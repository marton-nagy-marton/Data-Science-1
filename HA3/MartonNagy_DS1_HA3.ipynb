{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science 1 - Home Assignment 3\n",
    "\n",
    "**Author: MÃ¡rton Nagy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training set: 58, size of the test set: 25\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "prng = np.random.RandomState(20250317)\n",
    "\n",
    "url_data_on_github = 'https://raw.githubusercontent.com/divenyijanos/ceu-ml/refs/heads/2025/data/real_estate/real_estate.csv'\n",
    "real_estate_data = pd.read_csv(url_data_on_github)\n",
    "real_estate_sample = real_estate_data.sample(frac=0.2, random_state=prng)\n",
    "\n",
    "outcome = real_estate_sample['house_price_of_unit_area']\n",
    "features = real_estate_sample.drop('house_price_of_unit_area', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, outcome, test_size=0.3, random_state=prng)\n",
    "\n",
    "print(f\"Size of the training set: {len(X_train)}, size of the test set: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "**Description:** . Think about an appropriate loss function you can use to evaluate your predictive models. What is the risk (from a business perspective) that you would have to take by making a wrong prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateWeightedMAE(y_true, y_pred):\n",
    "    weight = 1/4\n",
    "    errors = y_pred - y_true\n",
    "    # for positive errors, the weight is one-quarter, for negative errors, the weight is three-quarters\n",
    "    # thus negative errors are three times as important as positive errors\n",
    "    loss = np.where(errors > 0, weight * np.abs(errors), (1 - weight) * np.abs(errors))\n",
    "    return np.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** As per the business description in the assignment, both over and underpredictions have a risk for the business. If the price is underpredicted, sellers will list their homes for lower prices than the true value, thus losing out on some profit. If the price is overpredicted, sellers may not be able to find any buyers - but they can react to this issue by lowering the listed price, and ultimately finding a buyer at the fair market price. Thus, I believe underpredictions are more risky for the business (as they cannot be reacted to). Also, the risk grows linearly as a function of the error in both directions (as the cost is equal to the error). Therefore, I believe the most appropriate loss function is a weighted mean absolute error, with higher weights for negative errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "**Description:** Build a simple benchmark model and evaluate its performance on the hold-out set (using your chosen loss function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a1ce5_row0_col0 {\n",
       "  background-color: #a50026;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a1ce5_row0_col1 {\n",
       "  background-color: #006837;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a1ce5\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a1ce5_level0_col0\" class=\"col_heading level0 col0\" >Train WMAE</th>\n",
       "      <th id=\"T_a1ce5_level0_col1\" class=\"col_heading level0 col1\" >Test WMAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a1ce5_level0_row0\" class=\"row_heading level0 row0\" >Benchmark</th>\n",
       "      <td id=\"T_a1ce5_row0_col0\" class=\"data row0 col0\" >5.25606</td>\n",
       "      <td id=\"T_a1ce5_row0_col1\" class=\"data row0 col1\" >4.31097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x20db730c7a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark = y_train.mean()\n",
    "\n",
    "class ResultCollector:\n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "        \n",
    "    def add_model(self, name, train_error, test_error):\n",
    "        \"\"\"Add or update a model's results.\"\"\"\n",
    "        self.results[name] = {\n",
    "            'Train WMAE': train_error,\n",
    "            'Test WMAE': test_error\n",
    "        }\n",
    "        return self.get_table()\n",
    "    \n",
    "    def get_table(self, style=True):\n",
    "        \"\"\"Get the results table with optional styling.\"\"\"\n",
    "        df = pd.DataFrame(self.results).T\n",
    "        if style:\n",
    "            return df.style.format(\"{:.5f}\").background_gradient(cmap='RdYlGn_r', axis=None)\n",
    "        return df\n",
    "    \n",
    "results = ResultCollector()\n",
    "\n",
    "results.add_model('Benchmark', calculateWeightedMAE(y_train, np.full(y_train.shape, benchmark)),\n",
    "                  calculateWeightedMAE(y_test, np.full(y_test.shape, benchmark)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** My simple benchmark model is just the mean of the train set. These results are not informative on their own (and there is no direct way to interpret them neither), but they will be a good baseline to compare the later models to. If we manage to achieve smaller WMAE values than the simple mean, than our models may have some business value to them. Note that the WMAE is smaller on the test set than on the training data, meaning that our model fits the test set better than what it was trained on - but this may be just by chance bacause of the random way we have constructed the sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "\n",
    "**Description:** Build a simple linear regression model using a chosen feature and evaluate its performance. Would you launch your evaluator web app using this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_99af9_row0_col0 {\n",
       "  background-color: #a50026;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_99af9_row0_col1 {\n",
       "  background-color: #006837;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_99af9_row1_col0 {\n",
       "  background-color: #eb5a3a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_99af9_row1_col1 {\n",
       "  background-color: #96d268;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_99af9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_99af9_level0_col0\" class=\"col_heading level0 col0\" >Train WMAE</th>\n",
       "      <th id=\"T_99af9_level0_col1\" class=\"col_heading level0 col1\" >Test WMAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_99af9_level0_row0\" class=\"row_heading level0 row0\" >Benchmark</th>\n",
       "      <td id=\"T_99af9_row0_col0\" class=\"data row0 col0\" >5.25606</td>\n",
       "      <td id=\"T_99af9_row0_col1\" class=\"data row0 col1\" >4.31097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_99af9_level0_row1\" class=\"row_heading level0 row1\" >Simple OLS</th>\n",
       "      <td id=\"T_99af9_row1_col0\" class=\"data row1 col0\" >5.09657</td>\n",
       "      <td id=\"T_99af9_row1_col1\" class=\"data row1 col1\" >4.57101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x20dbc797200>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "simple_ols_pipe = Pipeline([\n",
    "    ('select_cols', ColumnTransformer([('keep', 'passthrough', ['house_age'])])),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "simple_ols_pipe.fit(X_train, y_train)\n",
    "\n",
    "results.add_model('Simple OLS', calculateWeightedMAE(y_train, simple_ols_pipe.predict(X_train)),\n",
    "                  calculateWeightedMAE(y_test, simple_ols_pipe.predict(X_test)))\n",
    "results.get_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** I chose to train a model using the age of the house, as I thought this may be an important feature in determining the price. Interestingly though, even if this model performs better on the train set than the benchmark, it fits the test set worse, so there is not much added value in this feature if we want to do good predictions - thus I would not launch my app using this simple model. In addition, the train set WMAE is still higher than the test set, so there is a huge room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "\n",
    "**Description:** Build a multivariate linear model with all the meaningful variables available. Did it improve the predictive power?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_928d2_row0_col0 {\n",
       "  background-color: #a50026;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_928d2_row0_col1 {\n",
       "  background-color: #fee593;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_928d2_row1_col0 {\n",
       "  background-color: #c62027;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_928d2_row1_col1 {\n",
       "  background-color: #fdaf62;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_928d2_row2_col0 {\n",
       "  background-color: #1e9a51;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_928d2_row2_col1 {\n",
       "  background-color: #006837;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_928d2\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_928d2_level0_col0\" class=\"col_heading level0 col0\" >Train WMAE</th>\n",
       "      <th id=\"T_928d2_level0_col1\" class=\"col_heading level0 col1\" >Test WMAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_928d2_level0_row0\" class=\"row_heading level0 row0\" >Benchmark</th>\n",
       "      <td id=\"T_928d2_row0_col0\" class=\"data row0 col0\" >5.25606</td>\n",
       "      <td id=\"T_928d2_row0_col1\" class=\"data row0 col1\" >4.31097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_928d2_level0_row1\" class=\"row_heading level0 row1\" >Simple OLS</th>\n",
       "      <td id=\"T_928d2_row1_col0\" class=\"data row1 col0\" >5.09657</td>\n",
       "      <td id=\"T_928d2_row1_col1\" class=\"data row1 col1\" >4.57101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_928d2_level0_row2\" class=\"row_heading level0 row2\" >Multivariate OLS</th>\n",
       "      <td id=\"T_928d2_row2_col0\" class=\"data row2 col0\" >3.23100</td>\n",
       "      <td id=\"T_928d2_row2_col1\" class=\"data row2 col1\" >2.98373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x20dba8a3dd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_ols_pipe = Pipeline([\n",
    "    ('select_cols', ColumnTransformer([('keep', 'passthrough',\n",
    "                                        ['house_age', 'distance_to_the_nearest_MRT_station',\n",
    "                                         'number_of_convenience_stores', 'latitude', 'longitude'])])),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "multi_ols_pipe.fit(X_train, y_train)\n",
    "\n",
    "results.add_model('Multivariate OLS',\n",
    "                  calculateWeightedMAE(y_train, multi_ols_pipe.predict(X_train)),\n",
    "                  calculateWeightedMAE(y_test, multi_ols_pipe.predict(X_test)))\n",
    "results.get_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Including all possible features in the dataset (without any feature engineering) improved the model's performance significantly. The improvement is present for both the training and the test set, indicating that the model has actually learned some new patterns. Also, the train and test set WMAE metrics are now closer to each other, which indicates that there is less room for improvement from this model (but there is still some, so we are not yet ready for deployment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5\n",
    "\n",
    "**Description:** Try to make your model (even) better using the following approaches:\n",
    "\n",
    "- Feature engineering: e.g. including squares and interactions or making sense of latitude & longitude by calculating the distance from the city center, etc.\n",
    "- Training more flexible models: e.g. random forest or gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_eb7bc_row0_col0 {\n",
       "  background-color: #60ba62;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb7bc_row0_col1 {\n",
       "  background-color: #45ad5b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb7bc_row1_col0 {\n",
       "  background-color: #5ab760;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb7bc_row1_col1 {\n",
       "  background-color: #4bb05c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb7bc_row2_col0 {\n",
       "  background-color: #279f53;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb7bc_row2_col1 {\n",
       "  background-color: #1e9a51;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb7bc_row3_col0, #T_eb7bc_row7_col0 {\n",
       "  background-color: #006837;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb7bc_row3_col1 {\n",
       "  background-color: #a50026;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb7bc_row4_col0 {\n",
       "  background-color: #148e4b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb7bc_row4_col1, #T_eb7bc_row7_col1 {\n",
       "  background-color: #16914d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb7bc_row5_col0 {\n",
       "  background-color: #0c7f43;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb7bc_row5_col1, #T_eb7bc_row6_col1 {\n",
       "  background-color: #138c4a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb7bc_row6_col0 {\n",
       "  background-color: #0d8044;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb7bc_row8_col0 {\n",
       "  background-color: #0b7d42;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb7bc_row8_col1 {\n",
       "  background-color: #15904c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_eb7bc\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_eb7bc_level0_col0\" class=\"col_heading level0 col0\" >Train WMAE</th>\n",
       "      <th id=\"T_eb7bc_level0_col1\" class=\"col_heading level0 col1\" >Test WMAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_eb7bc_level0_row0\" class=\"row_heading level0 row0\" >Benchmark</th>\n",
       "      <td id=\"T_eb7bc_row0_col0\" class=\"data row0 col0\" >5.25606</td>\n",
       "      <td id=\"T_eb7bc_row0_col1\" class=\"data row0 col1\" >4.31097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb7bc_level0_row1\" class=\"row_heading level0 row1\" >Simple OLS</th>\n",
       "      <td id=\"T_eb7bc_row1_col0\" class=\"data row1 col0\" >5.09657</td>\n",
       "      <td id=\"T_eb7bc_row1_col1\" class=\"data row1 col1\" >4.57101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb7bc_level0_row2\" class=\"row_heading level0 row2\" >Multivariate OLS</th>\n",
       "      <td id=\"T_eb7bc_row2_col0\" class=\"data row2 col0\" >3.23100</td>\n",
       "      <td id=\"T_eb7bc_row2_col1\" class=\"data row2 col1\" >2.98373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb7bc_level0_row3\" class=\"row_heading level0 row3\" >Feature Engineered OLS</th>\n",
       "      <td id=\"T_eb7bc_row3_col0\" class=\"data row3 col0\" >0.00000</td>\n",
       "      <td id=\"T_eb7bc_row3_col1\" class=\"data row3 col1\" >27.37603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb7bc_level0_row4\" class=\"row_heading level0 row4\" >Feature Engineered LASSO</th>\n",
       "      <td id=\"T_eb7bc_row4_col0\" class=\"data row4 col0\" >2.18461</td>\n",
       "      <td id=\"T_eb7bc_row4_col1\" class=\"data row4 col1\" >2.40623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb7bc_level0_row5\" class=\"row_heading level0 row5\" >Basic Random Forest</th>\n",
       "      <td id=\"T_eb7bc_row5_col0\" class=\"data row5 col0\" >1.29234</td>\n",
       "      <td id=\"T_eb7bc_row5_col1\" class=\"data row5 col1\" >2.03972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb7bc_level0_row6\" class=\"row_heading level0 row6\" >Random Forest with CV</th>\n",
       "      <td id=\"T_eb7bc_row6_col0\" class=\"data row6 col0\" >1.45064</td>\n",
       "      <td id=\"T_eb7bc_row6_col1\" class=\"data row6 col1\" >2.08621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb7bc_level0_row7\" class=\"row_heading level0 row7\" >Basic XGBoost</th>\n",
       "      <td id=\"T_eb7bc_row7_col0\" class=\"data row7 col0\" >0.00040</td>\n",
       "      <td id=\"T_eb7bc_row7_col1\" class=\"data row7 col1\" >2.35427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb7bc_level0_row8\" class=\"row_heading level0 row8\" >XGBoost with CV</th>\n",
       "      <td id=\"T_eb7bc_row8_col0\" class=\"data row8 col0\" >1.25716</td>\n",
       "      <td id=\"T_eb7bc_row8_col1\" class=\"data row8 col1\" >2.28868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x20dbd43a630>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to calculate distance to city center\n",
    "def haversine(lat1, lon1, lat2=25.0330, lon2=121.5654):\n",
    "    \"\"\"\n",
    "    Calculate the great-circle distance between two points (lat1, lon1) and (lat2, lon2)\n",
    "    using the Haversine formula.\n",
    "\n",
    "    Parameters:\n",
    "    - lat1, lon1: Coordinates of the given point.\n",
    "    - lat2, lon2: Coordinates of New Taipei City center (default).\n",
    "\n",
    "    Returns:\n",
    "    - Distance in kilometers.\n",
    "    \"\"\"\n",
    "    R = 6371\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    distance = R * c\n",
    "\n",
    "    return distance\n",
    "\n",
    "# Function to convert float year to datetime\n",
    "def float_to_datetime(float_year):\n",
    "    \"\"\"\n",
    "    Convert a float year (e.g., 2013.250) to a datetime object.\n",
    "\n",
    "    Parameters:\n",
    "    - float_year (float): Year in decimal format.\n",
    "\n",
    "    Returns:\n",
    "    - datetime: Corresponding datetime object.\n",
    "    \"\"\"\n",
    "    year = int(float_year)\n",
    "    remainder = float_year - year\n",
    "\n",
    "    base_date = datetime(year, 1, 1)\n",
    "    days_in_year = (datetime(year + 1, 1, 1) - base_date).days\n",
    "\n",
    "    exact_date = base_date + timedelta(days=remainder * days_in_year)\n",
    "    \n",
    "    return exact_date\n",
    "\n",
    "# Applying the functions to the dataset\n",
    "X_train['distance_to_center'] = X_train.apply(lambda row: haversine(row['latitude'], row['longitude']), axis=1)\n",
    "X_test['distance_to_center'] = X_test.apply(lambda row: haversine(row['latitude'], row['longitude']), axis=1)\n",
    "X_train['transaction_date'] = X_train['transaction_date'].apply(float_to_datetime)\n",
    "X_test['transaction_date'] = X_test['transaction_date'].apply(float_to_datetime)\n",
    "\n",
    "X_train['year'] = X_train['transaction_date'].dt.year\n",
    "X_test['year'] = X_test['transaction_date'].dt.year\n",
    "X_train['month'] = X_train['transaction_date'].dt.month\n",
    "X_test['month'] = X_test['transaction_date'].dt.month\n",
    "\n",
    "# Setting numerical and categorical features\n",
    "# Latitude and longitude are still included, as they may hold other information as well\n",
    "# than distance to the city center (e.g., neighborhood).\n",
    "num_features = ['house_age', 'distance_to_the_nearest_MRT_station',\n",
    "                'number_of_convenience_stores', 'distance_to_center', 'latitude', 'longitude']\n",
    "cat_features = ['year', 'month']\n",
    "\n",
    "# Feature engineered OLS\n",
    "fe_ols_pipe = Pipeline([\n",
    "    ('preprocessor', ColumnTransformer([\n",
    "        ('num', 'passthrough', num_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), cat_features)\n",
    "    ])),\n",
    "    (\"2_degree_poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"drop_zero_variance\", VarianceThreshold()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "fe_ols_pipe.fit(X_train, y_train)\n",
    "\n",
    "results.add_model('Feature Engineered OLS', calculateWeightedMAE(y_train, fe_ols_pipe.predict(X_train)),\n",
    "                  calculateWeightedMAE(y_test, fe_ols_pipe.predict(X_test)))\n",
    "\n",
    "# Feature engineered LASSO\n",
    "fe_lasso_pipe = Pipeline([\n",
    "    ('preprocessor', ColumnTransformer([\n",
    "        ('num', StandardScaler(), num_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), cat_features)\n",
    "    ])),\n",
    "    (\"2_degree_poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"drop_zero_variance\", VarianceThreshold()),\n",
    "    ('regressor', LassoCV(random_state=prng, cv=5, max_iter=10000))\n",
    "])\n",
    "\n",
    "fe_lasso_pipe.fit(X_train, y_train)\n",
    "\n",
    "results.add_model('Feature Engineered LASSO', calculateWeightedMAE(y_train, fe_lasso_pipe.predict(X_train)),\n",
    "                  calculateWeightedMAE(y_test, fe_lasso_pipe.predict(X_test)))\n",
    "\n",
    "# Basic Random Forest\n",
    "rf_pipe = Pipeline([\n",
    "    ('select_cols', ColumnTransformer([('keep', 'passthrough', num_features+cat_features)])),\n",
    "    ('regressor', RandomForestRegressor(random_state=prng))\n",
    "])\n",
    "\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "\n",
    "results.add_model('Basic Random Forest', calculateWeightedMAE(y_train, rf_pipe.predict(X_train)),\n",
    "                    calculateWeightedMAE(y_test, rf_pipe.predict(X_test)))\n",
    "\n",
    "# Random Forest with CV\n",
    "rf_cv = RandomizedSearchCV(\n",
    "    estimator=rf_pipe,\n",
    "    param_distributions={\n",
    "        'regressor__n_estimators': [10, 50, 100, 200, 300],\n",
    "        'regressor__max_depth': [5, 10, 15, 20, None],\n",
    "        'regressor__min_samples_split': [2, 5, 10],\n",
    "        'regressor__min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    n_iter=250,\n",
    "    scoring=make_scorer(calculateWeightedMAE, greater_is_better=False),\n",
    "    n_jobs=-1,\n",
    "    cv=5,\n",
    "    refit=True,\n",
    "    random_state=prng\n",
    ")\n",
    "\n",
    "rf_cv.fit(X_train, y_train)\n",
    "\n",
    "results.add_model('Random Forest with CV', calculateWeightedMAE(y_train, rf_cv.predict(X_train)),\n",
    "                  calculateWeightedMAE(y_test, rf_cv.predict(X_test)))\n",
    "\n",
    "# Casting categorical features to category type\n",
    "X_train[cat_features] = X_train[cat_features].astype('category')\n",
    "X_test[cat_features] = X_test[cat_features].astype('category')\n",
    "\n",
    "# Basic XGBoost\n",
    "xgb_pipe = Pipeline([\n",
    "    ('select_cols', ColumnTransformer([('keep', 'passthrough', num_features+cat_features)])),\n",
    "    ('regressor', XGBRegressor(random_state=prng, enable_categorical=True))\n",
    "])\n",
    "\n",
    "xgb_pipe.fit(X_train, y_train)\n",
    "\n",
    "results.add_model('Basic XGBoost', calculateWeightedMAE(y_train, xgb_pipe.predict(X_train)),\n",
    "                  calculateWeightedMAE(y_test, xgb_pipe.predict(X_test)))\n",
    "results.get_table()\n",
    "\n",
    "# XGBoost with CV\n",
    "cv_xgb = RandomizedSearchCV(\n",
    "    estimator=xgb_pipe,\n",
    "    param_distributions={\n",
    "        'regressor__n_estimators': [10, 50, 100, 200, 300],\n",
    "        'regressor__max_depth': [5, 10, 15, 20],\n",
    "        'regressor__learning_rate': [0.01, 0.1, 0.3, 0.5],\n",
    "        'regressor__subsample': [0.5, 0.75, 1],\n",
    "        'regressor__colsample_bytree': [0.5, 0.75, 1],\n",
    "        'regressor__reg_alpha': [0, 0.1, 0.5, 1],\n",
    "        'regressor__reg_lambda': [0, 0.1, 0.5, 1]\n",
    "    },\n",
    "    n_iter=250,\n",
    "    scoring=make_scorer(calculateWeightedMAE, greater_is_better=False),\n",
    "    n_jobs=-1,\n",
    "    cv=5,\n",
    "    refit=True,\n",
    "    random_state=prng\n",
    ")\n",
    "\n",
    "cv_xgb.fit(X_train, y_train)\n",
    "\n",
    "results.add_model('XGBoost with CV', calculateWeightedMAE(y_train, cv_xgb.predict(X_train)),\n",
    "                  calculateWeightedMAE(y_test, cv_xgb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** I tried to improve my predictions using the following steps:\n",
    "\n",
    "- adding the distance to the city center (but still including the coordinates, as they may contain other relevant information as well, e.g. the neighbourhood, or proximity to certain points of interest);\n",
    "- adding dummy variables for the year and month of the transaction (as prices may fluctuate by time);\n",
    "- adding squared features and interactions.\n",
    "\n",
    "Having these, I trained a feature engineered OLS model, a feature engineered LASSO, a Random Forest and an XGBoost model (both with and without cross-validating the hyperparameters). It turns out that the feature engineered OLS model performs perfectly on the training data, but very bad on the test set, indicating a clear overfitting issue. The other models are more balanced in this sense. The best model (based on the test set WMAE) seems to be the Random Forest with deafault parameters, followed by the cross-validated XGBoost and LASSO. Thus, I will retrain the basic Random Forest and the CV XGBoost models on the full dataset, together with the less flexible multivariate OLS model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6\n",
    "\n",
    "**Description:** Rerun three of your previous models (including both flexible and less flexible ones) on the full train set. Ensure that your test result remains comparable by keeping that dataset intact. (Hint: extend the code snippet below.) Did it improve the predictive power of your models? Where do you observe the biggest improvement? Would you launch your web app now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the full training set: 389\n"
     ]
    }
   ],
   "source": [
    "real_estate_full = real_estate_data.loc[~real_estate_data.index.isin(X_test.index)]\n",
    "X_full = real_estate_full.drop('house_price_of_unit_area', axis=1)\n",
    "y_full = real_estate_full['house_price_of_unit_area']\n",
    "\n",
    "X_full['distance_to_center'] = X_full.apply(lambda row: haversine(row['latitude'], row['longitude']), axis=1)\n",
    "X_full['transaction_date'] = X_full['transaction_date'].apply(float_to_datetime)\n",
    "\n",
    "X_full['year'] = X_full['transaction_date'].dt.year\n",
    "X_full['month'] = X_full['transaction_date'].dt.month\n",
    "\n",
    "print(f\"Size of the full training set: {len(X_full)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_8df95_row0_col0 {\n",
       "  background-color: #60ba62;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8df95_row0_col1 {\n",
       "  background-color: #45ad5b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8df95_row1_col0 {\n",
       "  background-color: #5ab760;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8df95_row1_col1 {\n",
       "  background-color: #4bb05c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8df95_row2_col0 {\n",
       "  background-color: #279f53;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8df95_row2_col1, #T_8df95_row9_col1 {\n",
       "  background-color: #1e9a51;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8df95_row3_col0, #T_8df95_row7_col0 {\n",
       "  background-color: #006837;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8df95_row3_col1 {\n",
       "  background-color: #a50026;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8df95_row4_col0 {\n",
       "  background-color: #148e4b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8df95_row4_col1, #T_8df95_row7_col1 {\n",
       "  background-color: #16914d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8df95_row5_col0 {\n",
       "  background-color: #0c7f43;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8df95_row5_col1, #T_8df95_row6_col1, #T_8df95_row10_col1 {\n",
       "  background-color: #138c4a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8df95_row6_col0 {\n",
       "  background-color: #0d8044;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8df95_row8_col0, #T_8df95_row11_col0 {\n",
       "  background-color: #0b7d42;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8df95_row8_col1, #T_8df95_row11_col1 {\n",
       "  background-color: #15904c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8df95_row9_col0 {\n",
       "  background-color: #249d53;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8df95_row10_col0 {\n",
       "  background-color: #08773f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_8df95\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_8df95_level0_col0\" class=\"col_heading level0 col0\" >Train WMAE</th>\n",
       "      <th id=\"T_8df95_level0_col1\" class=\"col_heading level0 col1\" >Test WMAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_8df95_level0_row0\" class=\"row_heading level0 row0\" >Benchmark</th>\n",
       "      <td id=\"T_8df95_row0_col0\" class=\"data row0 col0\" >5.25606</td>\n",
       "      <td id=\"T_8df95_row0_col1\" class=\"data row0 col1\" >4.31097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8df95_level0_row1\" class=\"row_heading level0 row1\" >Simple OLS</th>\n",
       "      <td id=\"T_8df95_row1_col0\" class=\"data row1 col0\" >5.09657</td>\n",
       "      <td id=\"T_8df95_row1_col1\" class=\"data row1 col1\" >4.57101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8df95_level0_row2\" class=\"row_heading level0 row2\" >Multivariate OLS</th>\n",
       "      <td id=\"T_8df95_row2_col0\" class=\"data row2 col0\" >3.23100</td>\n",
       "      <td id=\"T_8df95_row2_col1\" class=\"data row2 col1\" >2.98373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8df95_level0_row3\" class=\"row_heading level0 row3\" >Feature Engineered OLS</th>\n",
       "      <td id=\"T_8df95_row3_col0\" class=\"data row3 col0\" >0.00000</td>\n",
       "      <td id=\"T_8df95_row3_col1\" class=\"data row3 col1\" >27.37603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8df95_level0_row4\" class=\"row_heading level0 row4\" >Feature Engineered LASSO</th>\n",
       "      <td id=\"T_8df95_row4_col0\" class=\"data row4 col0\" >2.18461</td>\n",
       "      <td id=\"T_8df95_row4_col1\" class=\"data row4 col1\" >2.40623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8df95_level0_row5\" class=\"row_heading level0 row5\" >Basic Random Forest</th>\n",
       "      <td id=\"T_8df95_row5_col0\" class=\"data row5 col0\" >1.29234</td>\n",
       "      <td id=\"T_8df95_row5_col1\" class=\"data row5 col1\" >2.03972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8df95_level0_row6\" class=\"row_heading level0 row6\" >Random Forest with CV</th>\n",
       "      <td id=\"T_8df95_row6_col0\" class=\"data row6 col0\" >1.45064</td>\n",
       "      <td id=\"T_8df95_row6_col1\" class=\"data row6 col1\" >2.08621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8df95_level0_row7\" class=\"row_heading level0 row7\" >Basic XGBoost</th>\n",
       "      <td id=\"T_8df95_row7_col0\" class=\"data row7 col0\" >0.00040</td>\n",
       "      <td id=\"T_8df95_row7_col1\" class=\"data row7 col1\" >2.35427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8df95_level0_row8\" class=\"row_heading level0 row8\" >XGBoost with CV</th>\n",
       "      <td id=\"T_8df95_row8_col0\" class=\"data row8 col0\" >1.25716</td>\n",
       "      <td id=\"T_8df95_row8_col1\" class=\"data row8 col1\" >2.28868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8df95_level0_row9\" class=\"row_heading level0 row9\" >Multivariate OLS (large N)</th>\n",
       "      <td id=\"T_8df95_row9_col0\" class=\"data row9 col0\" >3.11071</td>\n",
       "      <td id=\"T_8df95_row9_col1\" class=\"data row9 col1\" >2.90679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8df95_level0_row10\" class=\"row_heading level0 row10\" >Basic Random Forest (large N)</th>\n",
       "      <td id=\"T_8df95_row10_col0\" class=\"data row10 col0\" >0.90373</td>\n",
       "      <td id=\"T_8df95_row10_col1\" class=\"data row10 col1\" >2.06431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8df95_level0_row11\" class=\"row_heading level0 row11\" >XGBoost with CV (large N)</th>\n",
       "      <td id=\"T_8df95_row11_col0\" class=\"data row11 col0\" >1.19489</td>\n",
       "      <td id=\"T_8df95_row11_col1\" class=\"data row11 col1\" >2.31514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x20dbd55c560>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_full[cat_features] = X_full[cat_features].astype(int)\n",
    "X_test[cat_features] = X_test[cat_features].astype(int)\n",
    "\n",
    "multi_ols_pipe.fit(X_full, y_full)\n",
    "\n",
    "results.add_model('Multivariate OLS (large N)',\n",
    "                  calculateWeightedMAE(y_full, multi_ols_pipe.predict(X_full)),\n",
    "                  calculateWeightedMAE(y_test, multi_ols_pipe.predict(X_test)))\n",
    "\n",
    "rf_pipe.fit(X_full, y_full)\n",
    "\n",
    "results.add_model('Basic Random Forest (large N)', calculateWeightedMAE(y_full, rf_pipe.predict(X_full)),\n",
    "                  calculateWeightedMAE(y_test, rf_pipe.predict(X_test)))\n",
    "\n",
    "X_full[cat_features] = X_full[cat_features].astype('category')\n",
    "X_test[cat_features] = X_test[cat_features].astype('category')\n",
    "\n",
    "xgb_full = Pipeline([\n",
    "    ('select_cols', ColumnTransformer([('keep', 'passthrough', num_features+cat_features)])),\n",
    "    ('regressor', XGBRegressor(random_state=prng, enable_categorical=True,\n",
    "                               **{k.replace(\"regressor__\", \"\"): v for k, v in cv_xgb.best_params_.items()}))\n",
    "]).fit(X_full, y_full)\n",
    "\n",
    "results.add_model('XGBoost with CV (large N)', calculateWeightedMAE(y_full, xgb_full.predict(X_full)),\n",
    "                  calculateWeightedMAE(y_test, xgb_full.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** The training set performance improved for all three models with the addition of extra observations. Interestingly, having more data only improved (slightly) the test-set performance for the multivariate OLS model, the Random Forest and XGBoost performance remained more or less the same. This may indicate that these flexible models could already learn the main patterns on the smaller dataset, and there were no additional patterns to learn in the extra observations. As the test set performance could not be improved further, I believe these models are close to the best we can achieve, thus they are ready to be used for launching the app. For this, I would opt for the Basic Random Forest model, as it still has the lowest WMAE value on the larger sample. Before deployment, I would of course retrain this model using all available data (that is, the full `real_estate_data`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataanalysis_py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
